{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install -U \"transformers>=4.46.0\" \"datasets>=2.19.0\" \"accelerate>=0.30.0\" \\\n",
    "    \"peft>=0.11.0\" \"bitsandbytes>=0.45.0\" \"jinja2>=3.1.0\" \"tqdm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, re, json, time, random\n",
    "from decimal import Decimal, InvalidOperation\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "DATASET_NAME   = \"openai/gsm8k\"\n",
    "DATASET_CONFIG = \"main\"\n",
    "SPLIT          = \"train\"\n",
    "\n",
    "TEACHER_NAME   = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/gsm8k_cot_kd_ready\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_JSONL = os.path.join(OUTPUT_DIR, \"gsm8k_train_cot_kd_ready.jsonl\")\n",
    "OUT_META  = os.path.join(OUTPUT_DIR, \"run_meta.json\")\n",
    "\n",
    "# Self-consistency: K candidates per question\n",
    "K = 6\n",
    "\n",
    "# Adjust for GPU\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Accuracy-first generation\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMPERATURE    = 0.20\n",
    "TOP_P          = 0.90\n",
    "REPETITION_PENALTY = 1.08\n",
    "\n",
    "# Keep policy\n",
    "KEEP_ALL_CORRECT = False     \n",
    "MAX_COT_CHARS    = 1200       \n",
    "\n",
    "# Buffered write\n",
    "WRITE_EVERY_N_BATCH = 10\n",
    "FLUSH_AT_END = True\n",
    "\n",
    "# RANGE_END=None for full split.\n",
    "RANGE_START = 0\n",
    "RANGE_END   = None\n",
    "\n",
    "SEED = 3407\n",
    "\n",
    "# SEED & TORCH FLAGS\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# REGEX\n",
    "# Extract LAST \"#### number\" (supports commas/decimals/sign)\n",
    "ANS_RE = re.compile(r\"####\\s*([-+]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\")\n",
    "\n",
    "ROLE_LINE_RE = re.compile(r\"(?im)^\\s*(assistant|user|system)\\s*:?(\\s*)$\")\n",
    "\n",
    "# Remove obvious prompt-leak / meta lines (very important)\n",
    "LEAK_RE = re.compile(\n",
    "    r\"(?im)^\\s*(requirements:|rules:|example format:|format:|instructions:|\"\n",
    "    r\"end with|no extra text|do not include|start with:|output only|\"\n",
    "    r\"steps:|step\\s+\\d+\\s*:\\s*\\.\\.\\.)\\b.*$\"\n",
    ")\n",
    "\n",
    "# Minimal LaTeX wrappers & common commands\n",
    "LATEX_INLINE_RE  = re.compile(r\"\\\\\\((.*?)\\\\\\)\", re.S)\n",
    "LATEX_DISPLAY_RE = re.compile(r\"\\\\\\[(.*?)\\\\\\]\", re.S)\n",
    "DOLLAR_MATH_RE   = re.compile(r\"\\$\\$(.*?)\\$\\$|\\$(.*?)\\$\", re.S)\n",
    "\n",
    "# PROMPT (KD-ready, low leak risk)\n",
    "def build_messages_kd(question: str) -> List[Dict[str, str]]:\n",
    "    system = (\n",
    "        \"You are a precise math solver.\\n\"\n",
    "        \"Output ONLY the solution.\\n\"\n",
    "        \"Requirements:\\n\"\n",
    "        \"- Start with: Step 1:\\n\"\n",
    "        \"- Use short numbered steps with explicit arithmetic.\\n\"\n",
    "        \"- Do NOT repeat the question.\\n\"\n",
    "        \"- End with exactly: #### <number>\\n\"\n",
    "        \"- Do NOT include any other text.\\n\"\n",
    "    )\n",
    "    user = question.strip()\n",
    "    return [{\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "# HELPERS: number parsing / normalization (Decimal-safe)\n",
    "def extract_final_answer(text: str) -> Optional[str]:\n",
    "    matches = ANS_RE.findall(text)\n",
    "    if not matches:\n",
    "        return None\n",
    "    return matches[-1].replace(\",\", \"\").strip()\n",
    "\n",
    "def normalize_decimal_str(s: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Normalize numeric string with Decimal to avoid float issues.\n",
    "    - remove commas/spaces\n",
    "    - normalize unicode minus\n",
    "    - if integer => \"72\" (no .0)\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s).strip().replace(\",\", \"\").replace(\" \", \"\")\n",
    "    s = s.replace(\"−\", \"-\")\n",
    "    if s == \"\":\n",
    "        return None\n",
    "    try:\n",
    "        d = Decimal(s)\n",
    "    except (InvalidOperation, ValueError):\n",
    "        return None\n",
    "    if d == d.to_integral_value():\n",
    "        return str(int(d))\n",
    "    # keep a clean decimal form\n",
    "    return format(d.normalize(), \"f\").rstrip(\"0\").rstrip(\".\")\n",
    "\n",
    "def gold_from_gsm8k_answer(answer_text: str) -> Optional[str]:\n",
    "    return normalize_decimal_str(extract_final_answer(answer_text))\n",
    "\n",
    "# CLEANING / FORMAT ENFORCEMENT\n",
    "def _latex_to_plain(t: str) -> str:\n",
    "    # Strip wrappers\n",
    "    t = LATEX_INLINE_RE.sub(lambda m: m.group(1), t)\n",
    "    t = LATEX_DISPLAY_RE.sub(lambda m: m.group(1), t)\n",
    "    t = DOLLAR_MATH_RE.sub(lambda m: (m.group(1) or m.group(2) or \"\"), t)\n",
    "\n",
    "    # Convert common commands\n",
    "    t = re.sub(r\"\\\\frac\\{([^}]*)\\}\\{([^}]*)\\}\", r\"(\\1)/(\\2)\", t)\n",
    "    t = re.sub(r\"\\\\dfrac\\{([^}]*)\\}\\{([^}]*)\\}\", r\"(\\1)/(\\2)\", t)\n",
    "    t = t.replace(\"\\\\times\", \"×\").replace(\"\\\\cdot\", \"·\").replace(\"\\\\div\", \"÷\")\n",
    "    t = re.sub(r\"\\\\left|\\\\right\", \"\", t)\n",
    "    t = re.sub(r\"\\\\text\\{([^}]*)\\}\", r\"\\1\", t)\n",
    "    t = re.sub(r\"\\\\mathrm\\{([^}]*)\\}\", r\"\\1\", t)\n",
    "\n",
    "    # Remove remaining commands\n",
    "    t = re.sub(r\"\\\\[a-zA-Z]+\", \"\", t)\n",
    "    t = t.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    return t\n",
    "\n",
    "def _renumber_steps(t: str) -> str:\n",
    "    lines = t.splitlines()\n",
    "    out = []\n",
    "    step_no = 0\n",
    "    for ln in lines:\n",
    "        m = re.match(r\"^\\s*Step\\s+(\\d+)\\s*:\\s*(.*)$\", ln)\n",
    "        if m:\n",
    "            step_no += 1\n",
    "            out.append(f\"Step {step_no}: {m.group(2).strip()}\")\n",
    "        else:\n",
    "            out.append(ln)\n",
    "    return \"\\n\".join(out).strip()\n",
    "\n",
    "def enforce_step_format(t: str) -> str:\n",
    "    t = t.strip()\n",
    "\n",
    "    # 1) Drop role lines + leak/meta lines\n",
    "    kept = []\n",
    "    for ln in t.splitlines():\n",
    "        s = ln.strip()\n",
    "        if ROLE_LINE_RE.match(s):\n",
    "            continue\n",
    "        if LEAK_RE.match(s):\n",
    "            continue\n",
    "        kept.append(ln)\n",
    "    t = \"\\n\".join(kept).strip()\n",
    "\n",
    "    # 2) Hard cut: keep from first \"Step 1:\" onward (prevents prompt leak + question echo)\n",
    "    m = re.search(r\"(?m)^\\s*Step\\s+1\\s*:\", t)\n",
    "    if m:\n",
    "        t = t[m.start():].lstrip()\n",
    "\n",
    "    # 3) Keep only up to last ####\n",
    "    ans_matches = list(ANS_RE.finditer(t))\n",
    "    if ans_matches:\n",
    "        t = t[:ans_matches[-1].end()].strip()\n",
    "\n",
    "    # 4) Remove LaTeX\n",
    "    t = _latex_to_plain(t)\n",
    "\n",
    "    # 5) Normalize whitespace\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t).strip()\n",
    "\n",
    "    # 6) Renumber steps sequentially\n",
    "    t = _renumber_steps(t)\n",
    "\n",
    "    # 7) Length cap but keep #### line if present\n",
    "    if MAX_COT_CHARS and len(t) > MAX_COT_CHARS:\n",
    "        if \"####\" in t:\n",
    "            last_hash = t.rfind(\"####\")\n",
    "            start = max(0, last_hash - (MAX_COT_CHARS - 40))\n",
    "            t = t[start:].lstrip()\n",
    "        else:\n",
    "            t = t[:MAX_COT_CHARS].rstrip()\n",
    "\n",
    "    return t\n",
    "\n",
    "def is_valid_cot(t: str) -> bool:\n",
    "    if \"####\" not in t:\n",
    "        return False\n",
    "    if re.search(r\"(?m)^\\s*Step\\s+1\\s*:\", t) is None:\n",
    "        return False\n",
    "    if not t.lstrip().startswith(\"Step 1:\"):\n",
    "        return False\n",
    "    last = t.strip().splitlines()[-1].strip()\n",
    "    if not re.match(r\"^####\\s*[-+]?\\d+(?:,\\d{3})*(?:\\.\\d+)?\\s*$\", last):\n",
    "        return False\n",
    "    if re.search(r\"(?i)(end with|no extra text|example format|requirements:|rules:)\", t):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# JSONL writer\n",
    "def write_jsonl_buffer(path: str, buffer: List[Dict[str, Any]]) -> None:\n",
    "    if not buffer:\n",
    "        return\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for obj in buffer:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# LOAD DATA\n",
    "ds = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
    "data = ds[SPLIT]\n",
    "n_total = len(data)\n",
    "\n",
    "if RANGE_END is None:\n",
    "    RANGE_END = n_total - 1\n",
    "\n",
    "assert 0 <= RANGE_START < n_total\n",
    "assert 0 <= RANGE_END < n_total\n",
    "assert RANGE_START <= RANGE_END\n",
    "\n",
    "n_to_process = RANGE_END - RANGE_START + 1\n",
    "\n",
    "print(f\"Loaded {DATASET_NAME}/{DATASET_CONFIG} split={SPLIT} n={n_total}\")\n",
    "print(f\"Processing range: [{RANGE_START}, {RANGE_END}] -> {n_to_process} examples\")\n",
    "print(f\"Output JSONL: {OUT_JSONL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LOAD TEACHER (4-bit)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEACHER_NAME, use_fast=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# left padding is better for batched generation with different prompt lengths\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        TEACHER_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"sdpa\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    print(\"[OK] Loaded model with SDPA attention.\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] SDPA failed: {str(e)[:200]}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        TEACHER_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded. Device: {model.device}\")\n",
    "\n",
    "# MAIN LOOP\n",
    "start_time = time.time()\n",
    "processed = 0\n",
    "kept = 0\n",
    "buffer: List[Dict[str, Any]] = []\n",
    "\n",
    "num_batches = (n_to_process + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for batch_id, start_idx in enumerate(range(RANGE_START, RANGE_END + 1, BATCH_SIZE), start=1):\n",
    "    end_idx = min(start_idx + BATCH_SIZE, RANGE_END + 1)\n",
    "    batch = data.select(range(start_idx, end_idx))\n",
    "\n",
    "    questions = batch[\"question\"]\n",
    "    gold_raw  = batch[\"answer\"]\n",
    "    gold_nums = [gold_from_gsm8k_answer(a) for a in gold_raw]\n",
    "\n",
    "    # Build prompts\n",
    "    batch_messages = [build_messages_kd(q) for q in questions]\n",
    "    prompts = tokenizer.apply_chat_template(\n",
    "        batch_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    enc = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    attn_mask = enc[\"attention_mask\"]\n",
    "    prompt_lens = attn_mask.sum(dim=1).tolist()\n",
    "\n",
    "    # Repeat for K samples\n",
    "    input_ids_rep = input_ids.repeat_interleave(K, dim=0)\n",
    "    attn_mask_rep = attn_mask.repeat_interleave(K, dim=0)\n",
    "    prompt_lens_rep = [int(L) for L in prompt_lens for _ in range(K)]\n",
    "\n",
    "    # Generate\n",
    "    with torch.inference_mode():\n",
    "        seqs = model.generate(\n",
    "            input_ids=input_ids_rep,\n",
    "            attention_mask=attn_mask_rep,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "        )\n",
    "\n",
    "    # Decode completions only\n",
    "    raw_all = []\n",
    "    for i in range(seqs.size(0)):\n",
    "        L = prompt_lens_rep[i]\n",
    "        gen_tokens = seqs[i, L:]\n",
    "        raw = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "        raw_all.append(raw)\n",
    "\n",
    "    # Process each example\n",
    "    for j in range(len(questions)):\n",
    "        q = questions[j]\n",
    "        gold = gold_nums[j]\n",
    "\n",
    "        raws = raw_all[j*K:(j+1)*K]\n",
    "        comps = [enforce_step_format(r) for r in raws]\n",
    "\n",
    "        correct = []\n",
    "        for comp in comps:\n",
    "            if not is_valid_cot(comp):\n",
    "                continue\n",
    "            pred = normalize_decimal_str(extract_final_answer(comp))\n",
    "            if (pred is not None) and (gold is not None) and (pred == gold):\n",
    "                correct.append(comp)\n",
    "\n",
    "        processed += 1\n",
    "\n",
    "        if correct:\n",
    "            correct_sorted = sorted(correct, key=len)\n",
    "            kept_solutions = correct_sorted if KEEP_ALL_CORRECT else [correct_sorted[0]]\n",
    "\n",
    "            out_obj = {\n",
    "                \"id\": f\"gsm8k_train_{start_idx + j:05d}\",\n",
    "                \"dataset\": DATASET_NAME,\n",
    "                \"config\": DATASET_CONFIG,\n",
    "                \"split\": SPLIT,\n",
    "                \"question\": q,\n",
    "                \"gold_answer\": gold,\n",
    "                \"cot_solutions\": kept_solutions,\n",
    "                \"num_correct_in_K\": len(correct),\n",
    "                \"K\": K,\n",
    "                \"gen_params\": {\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"top_p\": TOP_P,\n",
    "                    \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "                    \"repetition_penalty\": REPETITION_PENALTY,\n",
    "                },\n",
    "            }\n",
    "            buffer.append(out_obj)\n",
    "            kept += 1\n",
    "\n",
    "    # Buffered write\n",
    "    if (batch_id % WRITE_EVERY_N_BATCH == 0) and buffer:\n",
    "        write_jsonl_buffer(OUT_JSONL, buffer)\n",
    "        buffer.clear()\n",
    "\n",
    "    # Periodic cleanup\n",
    "    if batch_id % 20 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Logging\n",
    "    if batch_id == 1 or batch_id % 10 == 0 or batch_id == num_batches:\n",
    "        elapsed = time.time() - start_time\n",
    "        ex_per_s = processed / max(elapsed, 1e-9)\n",
    "        keep_rate = kept / max(processed, 1)\n",
    "        done = (end_idx - RANGE_START)\n",
    "        remaining = n_to_process - done\n",
    "        eta_sec = remaining / max(ex_per_s, 1e-9)\n",
    "        print(\n",
    "            f\"[{end_idx:5d}/{RANGE_END+1}] \"\n",
    "            f\"batch={batch_id}/{num_batches} \"\n",
    "            f\"kept={kept}/{processed} ({keep_rate:.1%}) \"\n",
    "            f\"speed={ex_per_s:.2f} ex/s ETA={eta_sec/60:.1f}m\"\n",
    "        )\n",
    "\n",
    "# Final flush\n",
    "if FLUSH_AT_END and buffer:\n",
    "    write_jsonl_buffer(OUT_JSONL, buffer)\n",
    "    buffer.clear()\n",
    "\n",
    "# Save metadata\n",
    "meta = {\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"config\": DATASET_CONFIG,\n",
    "    \"split\": SPLIT,\n",
    "    \"teacher\": TEACHER_NAME,\n",
    "    \"range\": [RANGE_START, RANGE_END],\n",
    "    \"params\": {\n",
    "        \"K\": K,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"repetition_penalty\": REPETITION_PENALTY,\n",
    "        \"max_cot_chars\": MAX_COT_CHARS,\n",
    "        \"keep_all_correct\": KEEP_ALL_CORRECT,\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"processed\": processed,\n",
    "        \"kept\": kept,\n",
    "        \"keep_rate\": kept / max(processed, 1),\n",
    "    },\n",
    "    \"output_jsonl\": OUT_JSONL,\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "\n",
    "with open(OUT_META, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Done in {(time.time()-start_time)/60:.1f}m\")\n",
    "print(f\"  Kept: {kept}/{processed} ({kept/max(processed,1):.1%})\")\n",
    "print(f\"  Output: {OUT_JSONL}\")\n",
    "\n",
    "# Sanity check: print last sample\n",
    "try:\n",
    "    with open(OUT_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    if lines:\n",
    "        sample = json.loads(lines[-1])\n",
    "        print(f\"\\n[Sample] ID: {sample['id']}\")\n",
    "        print(f\"Question: {sample['question'][:120]}...\")\n",
    "        print(f\"Solution:\\n{sample['cot_solutions'][0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Sanity check error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
